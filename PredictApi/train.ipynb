{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spliting Data "
      ],
      "metadata": {
        "id": "ILBpg_LfEuW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RyYHHMJDN80Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pa3mP59eCyEJ"
      },
      "outputs": [],
      "source": [
        "!pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders\n",
        "splitfolders.ratio('/content/drive/MyDrive/BE project InceptionNet v4/Aug_Data', output=\"/content/drive/MyDrive/BE project InceptionNet v4/data\", seed=1337, ratio=(.8, 0.2)) "
      ],
      "metadata": {
        "id": "bsBeYUlFDhWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip install pycocotools --user"
      ],
      "metadata": {
        "id": "Uk3ZzBNjhJ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "FPrCZXW56J4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "Irlm_wJX9Toy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the libraries as shown below\n",
        "import tensorflow\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model\n",
        "import ntpath\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "nUVuBzcEEDr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# re-size all the images to this\n",
        "IMAGE_SIZE = [224, 224]\n",
        "\n",
        "train_path = '/content/drive/MyDrive/BE project InceptionNet v4/data/train'\n",
        "valid_path = '/content/drive/MyDrive/BE project InceptionNet v4/data/val'"
      ],
      "metadata": {
        "id": "6Hc6sziqE58l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using InceptionV3 as trained on ImageNet Without finetunig"
      ],
      "metadata": {
        "id": "Ni9gCSD7hsNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import the InceptionV3 library as shown below and add preprocessing layer to the front of InceptionV3\n",
        "# Here we will be using imagenet weights\n",
        "\n",
        "inception = InceptionV3(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)"
      ],
      "metadata": {
        "id": "ReP2Xj7fFJ8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# don't train existing weights\n",
        "for layer in inception.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "KfmDlo5RGJXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# useful for getting number of output classes\n",
        "folders = glob.glob('/content/drive/MyDrive/BE project InceptionNet v4/data/train/*')\n",
        "print(len(folders))"
      ],
      "metadata": {
        "id": "uVLtRxofh4xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building model with InceptionV3 with imagenet weights\")\n",
        "model = Sequential([\n",
        "    inception,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(rate=0.2),\n",
        "    Dense(len(folders), activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Hu-nfT1CiBHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tell the model what cost and optimization method to use\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "N0LfSCFdiFz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Image Data Generator to import the images from the dataset\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "metadata": {
        "id": "wGgJtTlPilxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you provide the same target size as initialied for the image size\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/BE project InceptionNet v4/data/train/',\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 50,\n",
        "                                                 class_mode = 'categorical')"
      ],
      "metadata": {
        "id": "n117utaFivUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/BE project InceptionNet v4/data/val/',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 50,\n",
        "                                            class_mode = 'categorical')"
      ],
      "metadata": {
        "id": "oOmSdOLXjHQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "today = datetime.datetime.now()\n",
        "\n",
        "print(today)"
      ],
      "metadata": {
        "id": "iElY3QyQ6hMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import pickle\n",
        "\n",
        "today = datetime.datetime.now()\n",
        "\n",
        "filepath = \"/content/drive/MyDrive/BE project InceptionNet v4/model2/model_fit_{epoch:02d}-{val_accuracy:.2f}.h5\"\n",
        "checkpoint1 = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "early = EarlyStopping(monitor=\"acc\", mode=\"max\", patience=15)\n",
        "\n",
        "callbacks_list = [checkpoint1, early] #early\n",
        "\n",
        "\n",
        "history = model.fit_generator(\n",
        "  training_set,\n",
        "  validation_data=test_set,\n",
        "  epochs=25,\n",
        "  steps_per_epoch=len(training_set),\n",
        "  validation_steps=len(test_set),\n",
        "  callbacks=callbacks_list\n",
        ")"
      ],
      "metadata": {
        "id": "0dGEkTn2jQIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EZFTkGbljhzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using InceptionV3 while fine tuning the top 2 blocks"
      ],
      "metadata": {
        "id": "JPbpnMBIXCn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
        "# the first 249 layers and unfreeze the rest:\n",
        "for layer in inception.layers[:249]:\n",
        "    layer.trainable = False\n",
        "for layer in inception.layers[249:]:\n",
        "    layer.trainable = True"
      ],
      "metadata": {
        "id": "57DU8quog3-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building model with InceptionV3 with imagenet weights\")\n",
        "model_finetuned = Sequential([\n",
        "    inception,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(rate=0.2),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# tell the model what cost and optimization method to use\n",
        "model_finetuned.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CwgvN6Pblgy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import pickle\n",
        "\n",
        "filepath = \"/content/drive/MyDrive/BE project InceptionNet v4/Models1/model_finetuned1_{epoch:02d}-{val_accuracy:.2f}.h5\"\n",
        "checkpoint1 = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "early = EarlyStopping(monitor=\"acc\", mode=\"max\", patience=15)\n",
        "\n",
        "callbacks_list = [checkpoint1, early] #early\n",
        "\n",
        "history = model_finetuned.fit_generator(\n",
        "  training_set,\n",
        "  validation_data=test_set,\n",
        "  epochs=20,\n",
        "  steps_per_epoch=len(training_set),\n",
        "  validation_steps=len(test_set), \n",
        "    callbacks=callbacks_list\n",
        ")\n",
        "\n",
        "with open('trainHistoryDict.txt', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)"
      ],
      "metadata": {
        "id": "eshZlNgAwpEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CQ75P9FFwx6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_finetuned = load_model('/content/drive/MyDrive/BE project InceptionNet v4/model2/model_fit_14-0.98.h5')"
      ],
      "metadata": {
        "id": "vCE8KI7Ra3fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n"
      ],
      "metadata": {
        "id": "uEelQV5Xeq64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import OpenCV\n",
        "import cv2\n",
        "\n",
        "# Utility\n",
        "import itertools\n",
        "import random\n",
        "from collections import Counter\n",
        "from glob import iglob\n",
        "\n",
        "test_dir='/content/drive/MyDrive/BE project InceptionNet v4/test/val'\n",
        "\n",
        "def load_image(filename):\n",
        "    img = cv2.imread(filename)\n",
        "    img = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]) )\n",
        "    img = img /255\n",
        "    \n",
        "    return img\n",
        "\n",
        "classes=['bacterial', 'fungal', 'healthy', 'hypersensitivity']\n",
        "\n",
        "def predict(image):\n",
        "    probabilities = best_model_finetuned.predict(np.asarray([img]))[0]\n",
        "    class_idx = np.argmax(probabilities)\n",
        "    \n",
        "    return {classes[class_idx]: probabilities[class_idx]}"
      ],
      "metadata": {
        "id": "a0duJMi_XgWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(20,20),text_size=15, norm=False, savefig=True): \n",
        "    # Create the confustion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
        "    n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
        "    \n",
        "    # Plot the figure and make it pretty\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
        "    fig.colorbar(cax)\n",
        "    \n",
        "    # Are there a list of classes?\n",
        "    if classes:\n",
        "        labels = classes\n",
        "    else:\n",
        "        labels = np.arange(cm.shape[0])\n",
        "            \n",
        "    # Label the axes\n",
        "    ax.set(title=\"Confusion Matrix\",\n",
        "         xlabel=\"Predicted label\",\n",
        "         ylabel=\"True label\",\n",
        "         xticks=np.arange(n_classes), # create enough axis slots for each class\n",
        "         yticks=np.arange(n_classes), \n",
        "         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
        "         yticklabels=labels)\n",
        "    \n",
        "    # Make x-axis labels appear on bottom\n",
        "    ax.xaxis.set_label_position(\"bottom\")\n",
        "    ax.xaxis.tick_bottom()\n",
        "    \n",
        "    ### Added: Rotate xticks for readability & increase font size (required due to such a large confusion matrix)\n",
        "    plt.xticks(rotation=70, fontsize=text_size)\n",
        "    plt.yticks(fontsize=text_size)\n",
        "    \n",
        "    # Set the threshold for different colors\n",
        "    threshold = (cm.max() + cm.min()) / 2.\n",
        "    \n",
        "    # Plot the text on each cell\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if norm:\n",
        "            plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "              size=text_size)\n",
        "        else:\n",
        "            plt.text(j, i, f\"{cm[i, j]}\",\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "              size=text_size)\n",
        "            \n",
        "    # Save the figure to the current working directory\n",
        "    if savefig:\n",
        "        fig.savefig(\"confusion_matrix.png\")"
      ],
      "metadata": {
        "id": "Xosj7jzAYKSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_test_samples=114"
      ],
      "metadata": {
        "id": "6E4WsyxqYeQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = best_model_finetuned.predict(test_set, num_of_test_samples // 50 + 1)\n",
        "y_pred = np.argmax(Y_pred, axis=1)"
      ],
      "metadata": {
        "id": "fHDOHv7KY6Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "classes=['bacterial', 'fungal', 'healthy', 'hypersensitivity']\n",
        "make_confusion_matrix(test_set.classes, y_pred,classes=classes)"
      ],
      "metadata": {
        "id": "-eK9TYXaZJGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path ='/content/drive/MyDrive/BE project InceptionNet v4/test/val/'\n",
        "PList=glob.glob('/content/drive/MyDrive/BE project InceptionNet v4/test/val/*')\n",
        "for filename in PList:\n",
        "    img = load_image(str(filename))\n",
        "    prediction = predict(img)\n",
        "    print(\"ACTUAL CLASS: %s, PREDICTED: class: %s, confidence: %f\" % (os.path.basename(filename), list(prediction.keys())[0], list(prediction.values())[0]))\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xrmEX92IZVco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oLQazWvWaIEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}