{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
    "%pip install ftfy regex tqdm\n",
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _imaging: Não foi possível encontrar o módulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mclip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n",
      "File \u001b[1;32mc:\\Users\\dudum\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\clip\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclip\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dudum\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\clip\\clip.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Union, List\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Compose, Resize, CenterCrop, ToTensor, Normalize\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[1;32mc:\\Users\\dudum\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\PIL\\Image.py:100\u001b[0m\n\u001b[0;32m     91\u001b[0m MAX_IMAGE_PIXELS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _imaging \u001b[38;5;28;01mas\u001b[39;00m core\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m __version__ \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(core, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCore version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(core,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPillow version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m         )\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _imaging: Não foi possível encontrar o módulo especificado."
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import clip\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "import os\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy\n",
    "\n",
    "print(numpy.__version__)\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo CLIP\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Configurar OpenAI para gerar descrições\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=\"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\",\n",
    ")\n",
    "\n",
    "def generate_class_descriptions(classes):\n",
    "    \"\"\" Gera descrições detalhadas para cada classe usando deepseek-v3 \"\"\"\n",
    "    descriptions = []\n",
    "    for cls in classes:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "            messages=[ \n",
    "                {\"role\": \"system\", \"content\": \"Descreva visualmente esta categoria de imagem.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Como é uma imagem da categoria {cls}?\"}\n",
    "            ]\n",
    "        )\n",
    "        descriptions.append(response.choices[0].message.content)\n",
    "    return descriptions\n",
    "\n",
    "def load_image_cv2(image_path):\n",
    "    \"\"\" Carrega uma imagem com OpenCV e converte para o formato RGB \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Erro ao carregar a imagem: {image_path}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # OpenCV carrega em BGR, converter para RGB\n",
    "    return Image.fromarray(image)\n",
    "\n",
    "def extract_clip_features(image_path):\n",
    "    \"\"\" Extrai embeddings CLIP da imagem \"\"\"\n",
    "    image = load_image_cv2(image_path)\n",
    "    image = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "    return image_features\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    \"\"\" Carrega imagens e categorias do dataset \"\"\"\n",
    "    dataset = {}\n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        class_dir = os.path.join(dataset_path, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            dataset[class_name] = [\n",
    "                os.path.join(class_dir, img) \n",
    "                for img in os.listdir(class_dir) \n",
    "                if img.lower().endswith(('.png', '.jpg', '.jpeg'))  # Filtra apenas imagens\n",
    "            ]\n",
    "    return dataset\n",
    "\n",
    "def refine_class_descriptions(dataset):\n",
    "    \"\"\" Gera descrições otimizadas com base nas imagens do dataset \"\"\"\n",
    "    refined_descriptions = {}\n",
    "    for class_name, image_paths in dataset.items():\n",
    "        class_images = torch.cat([extract_clip_features(img) for img in image_paths], dim=0)\n",
    "        avg_feature = class_images.mean(dim=0)\n",
    "        refined_descriptions[class_name] = avg_feature\n",
    "    return refined_descriptions\n",
    "\n",
    "def classify_with_clip(image_path, refined_descriptions):\n",
    "    \"\"\" Classifica uma imagem comparando com as descrições refinadas \"\"\"\n",
    "    image_features = extract_clip_features(image_path)\n",
    "    similarities = {cls: torch.cosine_similarity(image_features, desc.unsqueeze(0)).item() for cls, desc in refined_descriptions.items()}\n",
    "    predicted_class = max(similarities, key=similarities.get)\n",
    "    return predicted_class\n",
    "\n",
    "def evaluate_metrics(dataset, refined_descriptions):\n",
    "    \"\"\" Avalia várias métricas de desempenho do modelo \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for class_name, image_paths in dataset.items():\n",
    "        for img_path in image_paths:\n",
    "            # Classificar a imagem\n",
    "            predicted_class = classify_with_clip(img_path, refined_descriptions)\n",
    "            \n",
    "            # Adicionar a classe real e a prevista para cálculo das métricas\n",
    "            y_true.append(class_name)\n",
    "            y_pred.append(predicted_class)\n",
    "    \n",
    "    # Calculando as métricas\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "    \n",
    "    # Matriz de Confusão\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Plotando a matriz de confusão\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    plt.xlabel('Classe Predita')\n",
    "    plt.ylabel('Classe Real')\n",
    "    plt.title('Matriz de Confusão')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, precision, recall, f1, cm\n",
    "\n",
    "# Classes definidas\n",
    "def main():\n",
    "    dataset_path = \"dataset\"  # Pasta contendo imagens organizadas por classe\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    refined_descriptions = refine_class_descriptions(dataset)\n",
    "    \n",
    "    # Avaliar as métricas\n",
    "    accuracy, precision, recall, f1, cm = evaluate_metrics(dataset, refined_descriptions)\n",
    "    \n",
    "    print(f\"Acurácia: {accuracy:.2f}\")\n",
    "    print(f\"Precisão: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
